\chapter{Evaluierung und Ausblick}\label{ch:evalundausblick}
In diesem Kapitel soll evaluiert werden, inwiefern die automatische Testdatengenerierung in ihrem zum Zeitpunkt dieser Arbeit erreichten Stand in Hinblick auf Kapitel \ref{ch:sollzustand} die dort erarbeiteten Anforderungen und Ziele erfüllt und ob sie somit als Erfolg zu benennen ist. Weiterhin wird das Projekt auf mögliche Optimierungen analysiert und ein Ausblick für eine mögliche Zukunft der automatischen Testdatengenerierung bei \textit{FNT} gegeben.

Wie in den Anforderungen als \textbf{Ausführungszeitpunkt} erwünscht, werden sämtliche Testdaten vor den Tests generiert, sodass zum Zeitpunkt der Testausführung der Zugriff auf alle Daten möglich ist und die Tests in beliebiger Reihenfolge ausgeführt werden können.

Angestrebt wurde eine \textbf{Testdatenabdeckung} von 100\%, also eine automatische Generierung aller im Projekt \enquote{\ac{CIF} Automated Tests} vorkommenden Testdaten. Dies konnte aufgrund von Zeitmangel nicht erreicht werden. Es wurde sich bei der Umsetzung der Testdatengenerierung allerdings bewusst zuerst auf Testdaten für Hardware-\ac{E2E}-Tests fokussiert. Hierbei werden zum Zeitpunkt dieser Arbeit von zwölf in den Tests implementierten Hardware-Entitäten (Equipments, Subequipments) bei allen bis auf einer Entität Testdaten durch die automatische Testdatengenerierung generiert, was einer Abdeckung von circa 92\% entspricht. Werden weitere noch nicht betrachtete Testdaten wie Zonen oder Jobs miteingerechnet, ergibt sich eine grobe Testdatenabdeckung von 61\%. Die automatische Testdatengenerierung kann jedoch mithilfe der existierenden Methoden zur Datengenerierung relativ einfach erweitert werden, um in der Zukunft auch diese noch fehlenden Testdaten abzudecken.

Mit einer nicht vollständig erreichten Testdatenabdeckung geht einher, dass auch der \textbf{Automatisierungsgrad} zu diesem Zeitpunkt nicht so hoch ist wie in den Anforderungen angestrebt. Für die implementierten Entitäten der Testdatengenerierung ist es aber durchaus der Fall, das im Grunde keinerlei händische Schritte mehr nötig sind, um neue Testdaten zu erstellen. Mit künftiger Erweiterung der Testdatengenerierung kann also auch der Automatisierungsgrad immer weiter verbessert werden.

Im Zuge der \textbf{Dokumentation} wird in der \textit{Readme}-Datei des Projekts \enquote{\ac{CIF} Automated Tests} genau erläutert, wie die Testdatengenerierung abläuft und welche Schritte notwendig sind, um diese zu konfigurieren und zu erweitern. Um die einfache Weiterentwicklung der Testdatengenerierung für die als nächsten Schritt geplanten Telco-Entitäten zu ermöglichen, ist außerdem ein ausführliches Konzept vorhanden, welches von interessierten \textit{FNT}-Entwicklern jederzeit eingesehen und zur Orientierung verwendet werden kann.

Es wurde bewusst vermieden, tiefgreifende Änderungen an der Testumgebung vorzunehmen. Ein Beispiel hierfür ist die Entscheidung gegen ein Modifizieren von Chassis-Masterdaten durch das Hinzufügen einer Slot-Kompatibilität für die Modules. Sämtliche von der Testdatengenerierung erstellte Testobjekte werden nach der Testausführung auch wieder aus \textit{Command} gelöscht, womit die Anforderung zur \textbf{Umgebungsbereinigung} erfüllt ist.

Eine \textbf{Direkteinbindung} in das automatisierte Testprojekt wurde von Beginn an angestrebt und auch umgesetzt. Neben der eventuell nötigen oder auch gewünschten Anpassung der Konfiguration der Testdatengenerierung sind keine weiteren Schritte notwendig, um Testdaten generieren zu lassen.

Durch die vielen Abfragen von auf der zu testenden Instanz verfügbaren Typen und definierten Attributen wurde eine größtmögliche \textbf{Unabhängigkeit} von der darunterliegenden \textit{Command}-Instanz bewirkt. Die Testdatengenerierung arbeitet mit den Daten, die auf der Instanz auch wirklich existieren, und benötigt beispielsweise keine fest vorgeschriebenen Device Master, wie es bei den manuell gepflegten Testdaten der Fall war.

Das Messen der \textbf{Performance} der automatisierten Tests mit automatischer Testdatengenerierung im Vergleich zu Testdaten aus \ac{JSON}-Dateien ist schwer zu messen, da das Testprojekt kontinuierlich wächst und mit neuen Tests auch die Dauer der Testausführung immer länger wird und so nicht genau gesagt werden kann, welchen Teil der Laufzeit die Testdatengenerierung ausmacht. Eine ungefähre Schätzung kann gemacht werden, indem \textit{Jenkins}-Builds des \enquote{\ac{CIF} Automated Tests} Projekts vor und nach Implementierung der Testdatengenerierung für alle Equipment-Entitäten verglichen werden. Sowohl bei Builds, für welche die Equipment-Testdaten aus Dateien ausgelesen wurden, als auch für spätere Builds mit generierten Equipment-Testdaten befand sich die Gesamtlaufzeit der automatisierten Tests konsistent zwischen 20 und 30 Minuten. Die Schwankungen von bis zu zehn Minuten traten in beiden Fällen auf und sind eher der Geschwindigkeit der \ac{BGE}-Aufrufe vor allem beim Erstellen der Objekte in \textit{Command} zuzuordnen als dem Auslesen oder Generieren von Daten. Insgesamt werden durch die automatische Testdatengenerierung also keine wesentlichen Performance-Einbußen verbucht.

Die Testdaten und damit verbunden auch die Testdatengenerierung als Ganzes konnte gemäß den Anforderungen in ihrer \textbf{Komplexität} auf das Nötigste reduziert werden, indem nur die Attribute in der Datengenerierung berücksichtigt werden, welche auch tatsächlich in der Deltaberechnung eine Rolle spielen. Viele Entitäten besitzen noch eine Vielzahl weiterer Attribute, die Logik zur Wertegenerierung bräuchten, wenn sie alle in die Basisobjekte eingebunden würden, obwohl deren Vorhandensein im Delta-Use-Case irrelevant ist. Mit dem Auslassen unnötiger Attribute werden Testdaten nur so detailreich wie nötig generiert, was Ressourcen spart und die Übersichtlichkeit des Projekts verbessert. Die Anforderung nach möglichst geringer Komplexität wurde also erfüllt.

Zwar konnten die meisten Anforderungen erfolgreich erfüllt werden, jedoch bedeutet dies nicht automatisch, dass das umgesetzte Lösungskonzept für jede Anforderung die Optimallösung darstellt. Tatsächlich konnten während der Umsetzung des Konzepts einige mögliche Optimierungen identifiziert werden. Diese sollen im Folgenden dargestellt werden.

Eine der Stellen mit dem größten Optimierungspotential findet sich beim Erstellen der Basisobjekte beziehungsweise das Abfragen aller benötigten Attribute für diese Objekte. Momentan muss sich hierfür teils auf von \textit{Command} zurückgelieferte Fehlermeldungen gestützt werden, wobei diese nicht immer einheitlich formatiert sind und daher das fehlende Attribut nicht immer zuverlässig auslesbar ist. Dies wurde so implementiert, da nach Absprache mit dem PSE2-Entwicklerteam, welches für das \ac{CIF} verantwortlich ist, die \textit{cifEntityConfiguration} als der einzige Weg zur Beschaffung der Attribute für eine Entität identifiziert wurde. Bei einer späteren Recherche stellte sich allerdings heraus, dass es möglich ist, die \ac{BGE}-Dokumentation abzufragen. Diese enthält für alle \ac{REST}-Endpunkte der \ac{BGE} jeweils sämtliche mögliche Attribute samt zusätzlicher Informationen wie beispielsweise der Erforderlichkeit eines Attributs. Hierdurch könnten für alle Entitäten sowohl die benötigten \ac{NMS}- als auch \textit{Command}-Attribute abgefragt werden, ohne vorzeitig ein Objekt in \textit{Command} platzieren zu müssen. 

BEISPIEL BGE DOKU!

Eine weitere Optimierung könnte an der Utility-Klasse \textit{DataGeneratorUtils} vorgenommen werden. Generell gelten Utility-Klassen nicht als optimal, da sie beispielsweise das eigentliche Prinzip der Objektorientierung brechen oder (WEITERER GRUND + ZITAT). Statt einer Utility-Klasse könnte eine instanziierbare Klasse eingesetzt werden, welche von jeder \textit{DataGenerator}-Variante neu instanziiert und dabei auf die Entität zugeschnitten wird und für diese passend Werte generieren kann. Eine andere Herangehensweise, die Utility-Klasse zumindest übersichtlicher und kleiner zu halten, indem aus einer großen Klasse mehrere kleinere, dafür speziell für einen bestimmten Aufgabenbereich angepassten Utility-Klassen gestaltet werden. Beispiele hierfür sind Klassen wie \textit{DataGeneratorIDUtils} oder \textit{DataGeneratorQueryUtils}.

Mit Hinblick auf die für die automatische Testdatengenerierung formulierten Anforderungen kann das Projekt insgesamt trotz Optimierungspotential als erfolgreich betrachtet werden. Es können zum Zeitpunkt dieser Arbeit die meisten Testdaten für die automatisierten Tests des \ac{CIF} durch die automatische Testdatengenerierung erstellt werden. Diese bietet eine durch manuell niedergeschriebene Testdaten nicht erreichbare Flexibilität, vor allem, wenn es um das Testen verschiedener Instanzen geht, die sich eventuell in ihren vorhandenen Masterdaten oder Attributen unterscheiden. Trotz dieses Erfolgs zeigt die Umsetzung und der erreichte Stand des Projekts aber auch, dass das Implementieren einer automatischen Testdatengenerierung wie dieser einen sehr großen Aufwand bedeutet, welcher sich für andere Projekte bei \textit{Command} eventuell gar nicht lohnt. (REFERENZ AUF UMFRAGE) Bei der automatischen Testdatengenerierung verhält es sich etwas wie bei den automatisierten Tests selbst: Die erste Implementierung ist sehr aufwändig und ressourcenintensiv, ist diese jedoch erst einmal abgeschlossen, kann längerfristig konsistent automatisiert getestet werden, ohne händische Eingriffe vornehmen zu müssen, was den Testprozess beschleunigt und häufigere Testdurchläufe zulässt. Manuell erstellte Testdaten wird die hier präsentierte Testdatengenerierun allerdings wohl nie vollständig ersetzen, da der Implementierungsaufwand hierzu einfach zu groß ist.