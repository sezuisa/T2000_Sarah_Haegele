%!TEX root = ../../main.tex

\chapter{Ist-Analyse}\label{ch:istanalyse}
In diesem Kapitel wird der Status quo bezüglich des automatisierten Testens bei FNT analysiert und dargestellt. Es wird dabei ein besonderer Fokus auf das Projekt \enquote{CIF Automated Tests} gelegt, da das in Kapitel \ref{ch:sollzustand} dieser Arbeit erarbeitete Konzept zur automatischen Testdatengenerierung auf diesem Projekt basiert.

\section{Automatisiertes Testen bei FNT}\label{sec:autotestsfnt}
Genereller Aufbau von automatischen Testprojekten - alle folgen einer ähnlichen Struktur. Einbindung Jenkins, Testsuites über Gradle-Tasks - FNT Test Policy, Test Strategy

Agile Development
Continuous Integration

\section{Das Projekt \enquote{CIF Automated Tests}}\label{sec:ciftestprojekt}
Aufbau vom CIF-Testprojekt im Speziellen, Klassenstruktur, Testsuites

Das automatisierte Testen im Rahmen des Projekts \enquote{CIF Automated Tests} wurde im Jahr 2021 begonnen. Zuvor belief sich das Testen des \ac{CIF} ausschließlich auf manuell durchgeführte Tests, wodurch der Aufwand zum Testen des gesamten Systems sehr hoch und auch ein sehr häufiges und zügiges Testen wie in anderen bereits automatisierten Projekten nicht möglich war. Mit der Integration des \ac{CIF} in die Standardmodule von \textit{Command} bekommt dessen Zuverlässigkeit allerdings einen immer höheren Stellenwert, da das Modul so an eine deutlich größere Zahl von Kunden ausgeliefert wird. Fehler im \ac{CIF} könnten so zu einer Abwertung des gesamten Images der Software führen. Unter diesem Gesichtspunkt sollte mithilfe von automatisierten Tests eine Steigerung der Sicherheit und Qualität des \ac{CIF} erreicht werden.

Das Testprojekt selbst richtet sich nach den in Kapitel \ref{sec:autotestsfnt} beschriebenen vorgegebenen Richtlinien zum automatisierten Testen bei FNT. Getestet wird in zehn verschiedenen Testphasen, welche in ihrer Reihenfolge und Struktur der Philosophie der in Abbildung PYRAMID-REF dargestellten Pyramide für automatisierte Tests folgen. Diese Testphasen, auch Testsuites genannt, werden getrennt voneinander ausgeführt. Das Ausführen einer Testsuite erfolgt über das im Projekt integrierte Build-Automations-Tool \textit{Gradle} durch sogenannte \enquote{Tasks}. 

\begin{enumerate}
    \item \textbf{Suite \enquote{apiCommandAlive}}: Es wird ein schneller Check durchgeführt, um zu überprüfen, ob die zu testenden \textit{Command}-Instanz erreichbar ist. Schlägt dieser Check fehl, so wird der Build sofort abgebrochen, da die Erreichbarkeit der Instanz eine Voraussetzung zur Durchführung aller weiteren Tests ist.
    \item \textbf{Suite \enquote{apiPreconditions}}: In der Precondition-Suite werden einige für die weiteren Tests benötigte Objekte in \textit{Command} angelegt. Dies beinhaltet beispielsweise einen Job, welcher zur Steuerung des Datenintegrationsprozesses nötig ist, und eine sogenannte \enquote{Zone}, also ein virtueller Campus mit einem Gebäude, einer Etage und einem Raum. Diese Zone muss für spätere Tests vorhanden sein, um Geräte darin platzieren zu können.
    \item \textbf{Suite \enquote{apiSmoke}}: Die Smoke-Suite besteht aus Tests, welche grundlegende Funktionen von \textit{Command} beziehungsweise des \ac{CIF} auf korrekte Funktionalität prüfen. Hierzu gehört sowohl das ausgiebige Testen der Login-Funktion als auch ein erster Check, ob die Deltaberechnung grundsätzlich durchführbar ist. Schlagen Tests in der Smoke-Suite fehl, wird der Build sofort abgebrochen, da bei Fehlern in diesen grundlegenden Funktionen ein weiteres Testen von Komponenten, die oft auf diese Funktionen basieren, sinnlos wäre.
    \item \textbf{Suite \enquote{apiSetUpEnvironment}}: In dieser Suite wird eine Testumgebung für die darauffolgenden End-To-End-Tests geschaffen. Mithilfe von vordefinierten Testdaten werden Testobjekte so in \textit{Command} und in die \ac{NMS}-Tabelle platziert, dass alle möglichen Deltafälle berechnet werden und in den weiteren Tests getestet werden können.
    \item \textbf{Suite \enquote{apiRegression}}: Die Regression-Suite dient zur Durchführung von Integrationstests. Hierzu zählt beispielsweise das Erstellen von einzelnen Objekten in der \ac{NMS}-Tabelle.
    \item \textbf{Suite \enquote{apiRegressionE2E}}: Diese Suite beinhaltet funktionale End-To-End-Tests. Getestet wird hierbei, ob die Deltafälle korrekt berechnet wurden, ob die Deltas genehmigt werden können und in \textit{Command} korrekt synchronisiert werden. Aufgrund der hohen Anzahl von Tests in dieser Suite wurde sie in einzelne Komponenten aufgeteilt:
    \begin{description}
        \item[Hardware:] Hardware bezeichnet alle Gerätetypen, beispielsweise Chassis oder Switch Cabinets.
        \item[Zone:] Die Zonentests betreffen alle Arten von Zonen, zum Beispiel Gebäude oder Räume.
    \end{description}
    \item \textbf{Suite \enquote{uiRegression}}: Neben den Tests über die \textit{Command} \ac{API}, die sogenannte \ac{BGE}, werden in dieser Suite grundlegende Tests über die \ac{GUI} von \textit{Command} ausgeführt, um deren Funktionalität zu prüfen.
    \item \textbf{Suite \enquote{uiRegressionE2E}}: In der UI-E2E-Suite werden End-To-End-Tests über die \ac{GUI} durchgeführt. \ac{GUI}-Tests gibt es im Vergleich zu den \ac{API}-Tests sehr wenig, da diese aufwändig zu implementieren sind und relativ lange in der Testdurchführung dauern.
    \item \textbf{Suite \enquote{apiCleanEnvironment}}: Die CleanEnvironment-Suite dient zur Überprüfung, ob Objekte aus \textit{Command} auch wieder gelöscht werden können und bereinigt gleichzeitig die zu testende Instanz von allen bei der Testdurchführung erstellten Objekten.
    \item \textbf{Task \enquote{publishReportToTestRail}}: Der letzte Task bei der Testausführung ist kein Testtask. Stattdessen werden die Ergebnisse des Testlaufs an die Testdokumentationsplattform \textit{TestRail} übermittelt, wo sie grafisch aufbereitet einsehbar sind.
\end{enumerate}

- Tasks werden in einer Datei namens build.gradle definiert und in einer auf Groovy basierenden Skriptsprache geschrieben. Diese Datei und damit die Testtasks werden beim Erstellen eines Builds ausgeführt. Schlägt ein Test fehl, hat das ein Fehlschlagen des gesamten Builds zufolge.

\begin{lstlisting}[caption=Ein Beispiel eines Gradle Tasks zum Ausführen einer Testsuite, label=Gradle-Task-Code, language=Groovy]
    // Gradle-task example
\end{lstlisting}

Die Tests werden in Java implementiert und über die Java-Testplattform \textit{JUnit} ausgeführt. JUnit erlaubt das Verwenden von Annotationen an den einzelnen Tests, beispielsweise um diese einfach in Testsuites zu gruppieren. Gerade diese Annotation kann in den Gradle Tasks genutzt werden, um wie in Listing \ref{Gradle-Task-Code} alle vorhandenen Tests zu filtern und nur die Tests einer bestimmten Testsuite auszuführen. \cite{junit:2021}

\begin{lstlisting}[caption=Ein Test in JUnit mit Annotationen, label=JUnit-Test,language=Java]
    // JUnit Test mit Annotation
\end{lstlisting}

\section{Testdaten im Projekt \enquote{CIF Automated Tests}}\label{sec:testdatenIst}
\ac{JSON}-Files, statisch, unflexibel, Typen fest codiert, unübersichtlich (sehr viele Dateien)
Großer Nachteil: Schlechte Portabilität. Testdaten sind auf eine konkrete Testinstanz angepasst und können eventuell auf einer anderen nicht verwendbar sein (fehlende Typen) (QUELLE FÜR PORTABILITÄT)

In den einzelnen \ac{JSON}-Dateien werden die Daten in Form eines Arrays gehalten, wobei ein Arrayeintrag ein Testobjekt darstellt. Für jede Entität existieren mehrere \ac{JSON}-Dateien, um die Anforderungen an die Deltafälle abzudecken - so benötigt ein Objekt mit Fall \enquote{CREATE} beispielsweise nur Testdaten in einer Datei, die \ac*{NMS}-Testobjekte für die Entität sammelt, der Fall \enquote{UPDATE} aber schon Daten in zwei verschiedenen Dateien, da sowohl ein Testobjekt zum Einfügen in die \ac*{NMS}-Tabelle als auch eines direkt in \textit{Command} vonnöten ist. Im Folgenden ist ein Objekt der Entität Chassis beispielhaft in \ac{JSON}-Form angegeben.

\begin{lstlisting}[caption=Testdaten für ein Chassis in JSON-Form, label=JSON-Testdaten,language=json]
[
    {
        "cSourceId": "TEST_CHASSIS_UPDATE",
        "selectedSourceSystem": "CIF_AUTOMATED_TEST",
        "cSourceType": "OSN2500",
        "id": "TEST_CHASSIS_UPDATE",
        "sourceSystem": "CIF_AUTOMATED_TEST",
        "visibleId": "TEST_CHASSIS_UPDATE",
        "ipAddress": "10.122.77.02",
        "hwRevision": "HW Version6",
        "swRevision": "SW Version6",
        "remark": "C19640286: Test nms chassis UPDATE",
        "serialNo": "123456561",
        "createLinkDeviceMaster": {
          "linkedElid": "EKJAG5VN1YBKEH"
        },
        "createLinkZone": {
          "linkedElid": "7RJHP3VXHV4VJO"
        }
    }
]
\end{lstlisting}

Diese \ac{JSON}-Dateien werden in der Testsuite \enquote{apiSetUpEnvironment} zunächst geparst und die einzelnen Arrayeinträge mithilfe der Java Bibliothek \enquote{Jackson} zu Objekten von sogenannten \ac{POJO}-Klassen deserialisiert. Daraufhin muss bei jedem Objekt einzeln der Wert für die \ac{Elid} der verlinkten Zone abgeändert werden. Dies hat den Grund, dass die Zone, in welcher während eines Testlaufs Testobjekte platziert werden, nach jedem Testlauf gelöscht und daher immer wieder neu erstellt werden muss. Hierbei verändert sich auch immer die \ac{Elid} der Zone, da \ac{Elid}s einzigartig sind und für jedes neu erstellte Objekt in \textit{Command} generiert werden. Dieser Wert kann also nur zur Zeit der Testausführung korrekt gesetzt werden.

Sind für alle Objekte die Attribute mit korrekten Werten gefüllt, werden sie über Aufrufe von dafür vorgesehenen \ac{REST}-Endpunkten der \textit{Command} \ac{BGE} entweder in die \ac{NMS}-Tabellen gefüllt oder in \textit{Command} erstellt.

\section{Implementierung von Testfällen}\label{sec:testimplementierung}
Als Einarbeitung in das Testprojekt Implementierung einiger Testfälle. Dabei wurden Testdaten manuell erstellt, was die meiste Zeit in Anspruch nahm und zu sehr vielen Fehlern führte, die korrigiert werden mussten

- Tests haben auf einer älteren Version von Command nicht funktioniert, da dort einer der fest in den \ac{JSON} Dateien niedergeschriebenen Hardwaretypen gar nicht vorhanden war. Wie schon in Kapitel \ref{sec:testdatenIst} als Nachteil der bisherigen Herangehensweise an Testdaten angemerkt

Buch "QA durch Softwaretests", S. 67: Vorteile von Testdaten in externen Dateien (aber es dreht sich um sehr kleine Systeme)
S. 311: Manuelle Erstellung von Testdaten kann sehr aufwändig werden